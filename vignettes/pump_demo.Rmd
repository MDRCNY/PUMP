---
title: "Demonstration of the pum-p package"
output: 
    rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Demonstration of the pum-p package}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---


```{r initialize, include = FALSE}
library( pum )
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 4,
  fig.align = "center"
)
options(knitr.kable.NA = '')
library( tidyverse )
library( knitr )
theme_set( theme_minimal() )
```

# Demo of using the pum-p package for a simple power calculation


To illustrate the `pum-p` package we conduct a power analysis for a blocked cluster-randomized RCT (this is a three level design with the random assignment at level two).
In particular, we follow the general design of the Diplomas Now evaluation conducted by MDRC (see, e.g., https://www.mdrc.org/project/diplomas-now#overview).

As taken from the above site, Diplomas Now "is a secondary school model focused on meeting the holistic needs of all students in grades six through twelve. It is designed to be robust and intense enough to transform or turn around high-poverty and high-needs middle grade and high schools attended by many students who fall off the path to high school graduation. Diplomas Now combines programming developed by each of the three organizations that created it: Talent Development, City Year, and Communities In Schools."
Diplomas Now, with MDRC as a partner, was one of the first validation grants awarded as part of the i3 competition (administered by the federal Department of Education and was created by the American Recovery and Reinvestment Act of 2009).

For the experiment, "62 secondary schools in 11 school districts agreed to participate in this study between 2011 and 2013. 32 of these schools were randomly assigned to implement the Diplomas Now model while the other 30 schools were assigned to a control group, continuing their existing school programs or implementing other reform strategies of their choosing." (Details quoted from the "Addressing Early Warning Indicators: Interim Impact Findings from the Investing in Innovation (i3) Evaluation of DIPLOMAS NOW" report.)

The Diplomas Now study covered both middle and high schools, and was rolled out over a series of years.
The designers therefore blocked the schools by district, school type, and year of roll-out.
After having to drop some schools due to various reasons, the evaluators were left with 29 high schools and 29 middle schools grouped in 21 random assignment blocks.

<!-- KH comment: this seems to assume a decent level of familiarity with multi-level designs. I assume that is our audience, but would it be helpful to provide a quick review just in case? For example, do non-education fields talk about hierarchical designs with the same terminology? I still get confused as to the difference for example between a block and a cluster.

LWM: Not sure what you mean by the above comment-->
We have three levels of data: 21 blocks (level 3) of a few schools (level 2) in each block, with students (level 1) in the schools.
The schools are _clusters_, and are the units we are randomly assigning to treatment and control.
We randomly assign _within block_, meaning each block is in effect a mini-experiment with a pre-designated proportion of units treated.
Under our naming, this configuration is a three level design with randomization at level two, or a "d3.2" design.

In truth, the RA blocks are further nested in districts, but if we use fixed effects for the RA blocks, as the authors of the published report did, then we can ignore this 4th level of nesting.
In this case our _model_ would be fixed effects at level 3, including a treatment by block interaction term, a random intercept at level 2, and a constant treatment coefficient at level 2 (varying by level 3 blocks).
We represent this as "m3ff2rc".
The "3ff" means we are including block fixed effects and also an interaction of these fixed effects with treatment, allowing for each block to have its own estimated treatment impact.  We then average these impacts across the blocks.

The reason we need to account for multiple testing in this evaluation is we have 12 outcomes of interest.
We have three types of primary outcome (Attendance, Behavior, Course performance, called the "ABC's"), along with an ABC composite measure of an indicator of whether a student is above given thresholds on all of the first three measures.
Due to the grouped nature of the outcomes, we elect to do a power analysis separately for each group to control family-wise error rather than overall error.^[We note that there are different guidelines for when to adjust for multiple outcomes in education studies. For example, Schochet (2008) recommends organizing primary outcomes into domains, conducting tests on composite domain outcomes, and applying multiplicity corrections to composites across domains. The What Works Clearinghouse applies multiplicity corrections to findings within the same domain rather than across different domains. Our methods apply to either case.] 


# Power of the original design

To calculate power we need to establish the design of the study, the size of the study, and the expected relationships between covariates, outcomes, and units in the study.
All of these numbers have to be determined given resource limitations, or estimated using prior knowledge, pilot studies, or other sources of information.
Regarding size, we, as is typical of most power analysis packages, assume equal size blocks and clusters.
For the above, this gives about 3 clusters per block.
The report states there were 14,950 students, giving around 258 students per school (normally we would use the geometric means of clusters per block and students per cluster as our design parameters, but that information is not available in the report).

We next turn to generating values for the remaining parameters.
In particular, we need values for the $R^2$ of the possible covariates, the Intraclass Correlation Coefficient (ICC), and an estimate of treatment variation.
The report does not report these quantities, but it does mention covariate adjustment in the presentation of the model.
Given the outcomes, it is unlikely there are very predictive individual level covariates, but prior year attendance, etc., is likely to be highly predictive of school level rates.
We thus set $R^2_1 = 0.1$ and $R^2_2 = 0.5$ for all outcomes.
We assume 5 covariates at level 1 and 3 at level 2 (this decision, especially for level 1, usually does not matter much in practice, unless sample sizes are very small).

In a three level model, we need to allocate overall variation to levels 1, 2, and 3 via an ICC at level 2 and level 3.
We start with level 2 ICC of 0.05, and a level 3 ICC of 0.40.
We set a relatively high level 3 ICC to capture the impact of blocking, which is designed to isolate variation.
We initially assume no treatment variation (a common assumption in most power analyses).
For further discussion of selecting these parameters see, for example, *CITE Porter original paper*.

<!-- KH comment: this paragraph again assumes a fair amount of background knowledge, but probably OK

LWM: I agree; not sure how much more we would want to put in.-->
At this point we also need to specify the design and method of analysis.
The experimental design is dictated by what we plan to randomize and the structure of the data; we identified that above.
The report calls their model a "two-level" model, but this is not quite in alignment with the language of this package.
In particular, the report analysis include fixed effects for the randomization block in their second level.
These fixed effects are accounting for variation at level 3.
We slightly depart from the report in that we are assuming a model with an interaction between treatment and these fixed effects, while the report has a constant treatment coefficient, resulting in a precision weighted average of impact across the randomization blocks.
This difference is unlikely to matter much in practice.

For illustration, we select attendance as our outcome group.
We have three different attendance measures, so we need to adjust across 3 outcomes.
We would then specify the size of our treatment impact, in effect size units, for each of our 3 outcomes.
We will initially assume a modest effect of 0.10 for all three outcomes; we will later explore power to detect effects of other sizes.

Within the attendance group, we also have to specify how correlated our test statistics are likely to be.
As a rough proxy, we would use the correlation of the outcomes at the level of randomization; in our case this would be how correlated average attendance measures would be across all schools.
We believe the attendance measures would be fairly related, so we select `rho = 0.40` for all pairs.

So far we have initial values for all our needed parameters.
Of course, in a full power analysis, we would explore ranges of values to see how power changes across a range of specifications; we discuss this further below.
We could also specify different values for the $R^2$s and $ICC$s for the different outcomes, if we thought they had different characteristics; for simplicity we do not do this here.
Similarly, we might specify different pairwise correlations between the different outcomes, providing a matrix of $\rho$s rather than a single value; again, for simplicity, we do not do that here.

Given a specific set of parameters, we calculate the power of our design using `pump_power()` as follows:

```{r setup}
library(pum)

p <- pump_power( design = "d3.2_m3ff2rc", # choice of design and analysis strategy
            MTP = "Bonferroni", # multiple testing procedure
            MDES = 0.10, # assumed effect size
            M = 3, # number of outcomes
            J = 3, # number of schools/block
            K = 21, # number RA blocks
            nbar = 258, # average number of students per school
            Tbar = 0.50, # prop Tx
            alpha = 0.05, # significance level
            numCovar.1 = 5, numCovar.2 = 3, # number of covariates per level
            R2.1 = 0.1, R2.2 = 0.7,
            ICC.2 = 0.05, ICC.3 = 0.4,
            rho = 0.4 ) # how correlated outcomes are
```

```{r echo = FALSE}
kable(p, digits=2)
```


*Results.* The first columns are the individual power for the 3 outcomes---they are the same since we assumed the same effect sizes for all three (MDES).  We are seeing around 80% power for this particular configuration for a MDES of 0.10. The `indiv.mean` is just the mean power across our 3 outcomes.
The first row is power without adjustment, and the second row has our power with the listed testing correction.

The next columns show our different multi-outcome definitions of power.
In particular, `min1` and `min2` show the chance of rejecting at least 1 or 2 of our hypotheses, respectively.
The `complete` is rejecting all 3.
The package does not show power for these without adjustment for multiple testing, as that power would be grossly inflated and is meaningless.

While the Bonferroni correction does substantially diminish individual power, we still have more than an 80% chance of rejecting at least 1 null of our 3 outcomes: while our study will not be well powered for any individual effect, it is more powered than we might expect to detect _some_ effect, even when using the very conservative correction procedure of Bonferroni.


<!--KP comment: I wonder if one slight modification to the set-up is to walk through the recommendations for how an analyst should make decisions about assumptions and run one scenario that they might come to - which you have done above but then illustrate the impact of other assumptions. You've done this but we perhaps spell it out more and include more discussion of the impact different values of different parameters can have.

LWM: Does the following do this?  I am not sure what you mean here. -->

Given the above, we might wonder how things may change if we change some of our parameters.
We can do this with `update()`.
For example, here we examine what happens if the ICCs are more equally split across level 2 and level 3 are both 0.20:
```{r}
p_b <- update( p, ICC.2 = 0.20, ICC.3 = 0.25 )
p_b
```

Clearly, the assumption of our variation primarily being captured in level 3 is important for our power.

For reference, we can always check a design with `print_design()`:

```{r}
print_design(p_b) 
```


# Examining other correction procedures

<!-- KH comment: as a standalone vignette, the user does not know how these things are being calculated, i.e. that these values are generated via simulation, so that might be a bit surprising ot them!

LWM: But I think  that is ok?
-->
It is easy to rerun the above using the Westfall-Young Stepdown correction procedure (this procedure is much more computationally intensive to run), or other procedures of interest.  In fact, you can provide `pump_power()` a list of procedures to compare everything easily.  If you provide a list, the package will re-run the simulation for each item on the list behind the scenes, so the overall method call can get computationally intensive.
Here we update, again, with a full list. We could also have simply provided this list with `pump_power()` initially.

```{r othercorrections, cache=TRUE}
# p2 <- update( p, MTP = c("Bonferroni", "Holm", "WY-SD") )
# TODO
p2 <- update( p, MTP = c("Bonferroni", "Holm", "WY-SD"), B = 100)
```

```{r echo = FALSE}
kable(p2)
```

Due to the correlation in our outcomes (`rho = 0.4`), we are seeing elevated power due to using a more sophisticated (and less conservative) correction approach for each individual test.  We do not see elevated rates for min-1 power, interestingly.
Accounting for the correlation of the test when doing test correction can drive some power (indivdual power) up, but on the flip side min-1 power can be driven down as the lack of independence between tests gives fewer chances for a significant result.
See *CITE porter* for further discussion; while Porter (2017) focuses on a single experimental design, the lessons learn apply to all designs as the only difference between the designs is in how we calculate the raw distribution of test statistics.



# Alternative methods of estimation

There are usually a range of modeling choices one might bring to a given experimental design.
For example, for multisite experiments ("d2.1" designs), *CITE Miratirx and Weiss* identify 15 different estimation strategies.
Different choices here can imply different targeted estimands, and these differences can impact power.
In particular, if one is using a method that targets a superpopulation average vs. a finite sample average, power can be substantially different.

In `pum-p` these choices are represented by the different modeling choices one can specify as part of the `design` parameter.
For our context, for example, we could use a random effects model at level 3 instead of a fixed effects model, setting `design = "d3.2_m3rr2rc"` instead of `"d3.2_m3ff2rc"`.
Random effects models allow for level 3 covariates, which we would need to specify via `numCovar.3` and `R2.3` to capture how many there are and how predictive they are:

```{r}
p3 <- update( p, design="d3.2_m3rr2rc", numCovar.3 = 3, R2.3 = 0.40 )
p3
```


# Exploring parameter combinations

<!--KP comment: As note earlier, I am wondering if we might expand this section more, looking at the impact of different assumptions across (1) R2; (2) ICC; (3) rho; (4) number of outcomes with impacts; and maybe (5) adjustment procedures. We can include plots from the plot functions we add to our package (or to the grid functions). 
-->

To explore sensitivity to different design parameters, we can call `pump_power_grid`, which will calculate power on all combinations of a set of passed parameter values.
For some discussion of what parameters will affect power more generally, see <<PowerUp paper?>>.
For discussion of how design parameters can affect the overall power in the multiple testing context, especially with regards to the overall power measures such as min1 or complete power, see the discussion in <<Kristen paper>>; the findings there are general, as they are a function of the final distribution of test statistics.
The key insight into the simulation approach is that power is a function of the individual-level standard errors and degrees of freedom, and how correlated the test statistics are; once we have these elements, regardless of the design, we can proceed.

To illustrate, we consider two common areas of exploration: Intraclass Correlation Coefficients (ICCs),  the correlation of test statistics, and the assumed number of non-zero effects.

## Exploring the impact of the ICC

We might not be sure what our ICCs are at level 2 and level 3, and want to ensure our power is sufficient across a range of plausible values.
We can explore a range of options for both ICCs, but use all the other parameters from before.
The `update_grid()` call makes this straightforward: we pass our baseline scenario along with lists of parameters to additionally explore:

```{r, cache=TRUE, fig.height=3, fig.width=7}
grid <- update_grid( p,
            ICC.2 = seq( 0, 0.3, 0.05 ),
            ICC.3 = seq( 0, 0.60, 0.20 ),
            tnum = 5000 ) 

grid$ICC.3 = as.factor( grid$ICC.3 )
grid = filter( grid, MTP == "Bonferroni" )
ggplot( grid, aes( ICC.2, min1, group = ICC.3, col = ICC.3 ) ) +
  geom_line() + geom_point()
```

We see that higher ICC.2 radically reduces power to detect anything (this makes sense: we are putting the variation right at the level of treatment assignment).
The ICC.3 parameter, by contrast, appears to do nothing.

**TODO: Check if this is indeed correct for ICC.3**

We reduced the number of permutations (to 5000 via `tnum`) to speed up computation.
As `tnum` shrinks, we will get only rough estimates of power, but even these rough estimates can help us determine trends.
Also note that the current implementation of the `grid` methods do not allow varying MDES vectors to keep package syntax simpler.

## Exploring the impact of the rho

The correlation of test statistics, $\rho$, is a critical parameter for how power will play out across the multiple tests.  For example, if we use Westfall-Young, the correlation will improve our individual power, and reduce our min1 power.  We might not know what will happen to min2 power: on one hand, correlated statistics make individual adjustment less severe, and on the other correlation means we succeed or fail all together.  We can explore this relatively easily by letting `rho` vary as so:

```{r, cache=TRUE, fig.height=3, fig.width=7}
grid <- update_grid( p,
            MTP = c( "Bonferroni", "WY-SS" ),
            rho = c( 0, 0.15, 0.3, 0.45, 0.6 ),
            tnum = 500,
            #### TODO
            # B = 10000
            B = 1000
            ) 
gridL = filter( grid, MTP != "None" ) %>%
  pivot_longer( cols=c(indiv.mean, min1, min2, complete),
                names_to="definition", values_to="power" ) %>%
  mutate( definition = factor( definition,
                               levels = c("indiv.mean", "min1", "min2", "complete" ) ) )

ggplot( gridL, aes( rho, power, col=MTP ) ) +
  facet_grid( . ~ definition ) +
  geom_line() + geom_point() +
  geom_hline( yintercept =0.80 ) + theme_minimal()
```

Across a very broad range of rho, we will still have good min1 power.  The impact on individual adjustment is flat, as anticipated. Complete power climbs as correlation increases, min-2 power is generally unchanged.

Interestingly, the benefit of the Westfall-Young single-step procedure is minimal in this case.


## Exploring the impact of null outcomes

We finally explore varying the number of zeros in our outcomes.  For illustration we also expand the number of outcomes to 5.
The tools are the same as before:

```{r, cache=TRUE, fig.width = 7, fig.height = 2.5 }
grid <- update_grid( p,
            numZero = 0:4,
            M = 5 )
gridL = filter( grid, MTP != "None" ) %>%
  pivot_longer( cols=c(indiv.mean, min1, min2, complete),
                names_to="definition", values_to="power" ) %>%
  mutate( definition = factor( definition, levels = c("indiv.mean", "min1", "min2", "complete" ) ) )

ggplot( gridL, aes( numZero, power ) ) +
  facet_grid( . ~ definition ) +
  geom_line() + geom_point() +
  geom_hline( yintercept =0.80 ) + theme_minimal()

```



 
# Calculating MDES 

We can use `pum-p` to calculate MDESes as well as power.
To identify the MDES of a given design we use the `pump_mdes` method. Along with the correction procedure and the design parameters discussed above, you also need to specify the target power, the type of power, and the tolerance of the search algorithm. 

Here, for example, we find the MDES for obtaining 80% individual power using the Holm procedure if we had 4 schools per block instead of 3:

```{r MDEScalc, cache=TRUE}
m <- pump_mdes(
           design = "d3.2_m3ff2rc",
           MTP = "Holm",
           target.power = 0.80, power.definition = "D1indiv", tol = 0.01,
           M = 3, J = 4, K = 21, nbar = 258,
           Tbar = 0.50, alpha = 0.05, numCovar.1 = 5, numCovar.2 = 3,
           R2.1 = 0.1, R2.2 = 0.7, ICC.2 = 0.05, ICC.3 = 0.4, rho = 0.4 )
```

```{r echo = FALSE}
kable(m, digits=2)
```

This method does a search, looking for the impact (assumed shared across all outcomes) to achieve the desired level of power. The tolerance controls how close the final estimated power should be to the target power; lower tolerance will give more exact results (and take more computational time).

If we are looking for the MDES for "min1" power (i.e., what effect do we have to assume across all observations such that we will find some significant result with 80% power), we have a smaller MDES:

```{r MDEScalcmin1, cache=TRUE}
mdes <- update( m, power.definition = "min1" )
```

```{r echo = FALSE}
kable(mdes, digits=2)
```


# Determining necessary sample size

For our design we might want to determine the needed number of students/school, number of schools, or number of blocks needed. The `pump_sample` method will search over any one of these, as requested.

Here we see how many schools are needed to achieve a MDES of 0.05 for complete power (so how many schools are needed to have 80% chance of finding all three outcomes significant, if all outcomes had a true effect size of 0.10).

```{r samplesizecalc, cache=TRUE}
smp <- pump_sample(
  design = "d3.2_m3ff2rc",
  MTP = "Bonferroni",
  typesample = "J",
  target.power = 0.80, power.definition = "complete", tol = 0.01,
  MDES = 0.10, M = 3, nbar = 258, K = 21,
  Tbar = 0.50, alpha = 0.05, numCovar.1 = 5, numCovar.2 = 3,
  R2.1 = 0.1, R2.2 = 0.7, ICC.2 = 0.05, ICC.3 = 0.40, rho = 0.4,
  just.result.table = FALSE
)
```

```{r echo = FALSE}
kable(smp)
```

We see we need only a modest increase in the average number of schools per randomization block.
We recommend checking the mdes and sample-size calculators as the estimation error combined with the search can give results a bit off the target in some cases.
Check by running the found design through `pump_power` to see if we recover or originally targeted power (we can use `update()` again for this):

```{r samplesizeverify, cache=TRUE}
# p_check <- update( smp, type="power", tnum = 100000 )
# TODO
p_check <- update( smp, type="power", tnum = 100 )
```

```{r echo = FALSE}
summary( p_check )
```

We can also look at the power curve to assess how sensitive power is to our sample size:

```{r, fig.height = 3.5, fig.width=5}
plot_power_curve( smp )
```

We see that each change in our number of schools per cluster changes power considerably.
We note that in practice, due to the number of schools within cluster needing to be whole, these power calculations are approximate.

