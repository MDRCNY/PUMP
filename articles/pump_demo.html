<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Demonstration of the PUMP package • PUMP</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Demonstration of the PUMP package">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">PUMP</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.4</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/pump_demo.html">Demonstration of the PUMP package</a></li>
    <li><a class="dropdown-item" href="../articles/pump_sample_demo.html">Demo and discussion of the pump_sample method</a></li>
    <li><a class="dropdown-item" href="../articles/pump_simulate.html">Demo of simulating multi-level data</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/MDRCNY/PUMP/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">



<script src="pump_demo_files/kePrint-0.0.1/kePrint.js"></script><link href="pump_demo_files/lightable-0.0.1/lightable.css" rel="stylesheet">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Demonstration of the PUMP package</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/MDRCNY/PUMP/blob/main/vignettes/pump_demo.Rmd" class="external-link"><code>vignettes/pump_demo.Rmd</code></a></small>
      <div class="d-none name"><code>pump_demo.Rmd</code></div>
    </div>

    
    
<center>
<img src="../reference/figures/pump_icon.png" alt="pump-icon" width="100">
</center>
<div class="section level2">
<h2 id="abstract">Abstract<a class="anchor" aria-label="anchor" href="#abstract"></a>
</h2>
<p>For randomized controlled trials (RCTs) with a single intervention
being measured on multiple outcomes, researchers often apply a multiple
testing procedure (such as Bonferroni or Benjamini-Hochberg) to adjust
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values.
Such an adjustment reduces the likelihood of spurious findings, but also
changes the statistical power, sometimes substantially, which reduces
the probability of detecting effects when they do exist. However, this
consideration is frequently ignored in typical power analyses, as
existing tools do not easily accommodate the use of multiple testing
procedures. We introduce the <code>PUMP</code> <code>R</code> package as
a tool for analysts to estimate statistical power, minimum detectable
effect size, and sample size requirements for multi-level RCTs with
multiple outcomes. Multiple outcomes are accounted for in two ways.
First, power estimates from <code>PUMP</code> properly account for the
adjustment in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
from applying a multiple testing procedure. Second, as researchers
change their focus from one outcome to multiple outcomes, different
definitions of statistical power emerge. <code>PUMP</code> allows
researchers to consider a variety of definitions of power, as some may
be more appropriate for the goals of their study. The package estimates
power for frequentist multi-level mixed effects models, and supports a
variety of commonly-used RCT designs and models and multiple testing
procedures. In addition to the main functionality of estimating power,
minimum detectable effect size, and sample size requirements, the
package allows the user to easily explore sensitivity of these
quantities to changes in underlying assumptions.</p>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The <code>PUMP</code> <code>R</code> package fills in an important
gap in open-source software tools to design multi-level randomized
controlled trials (RCTs) with adequate statistical power. With this
package, researchers can estimate statistical power, minimum detectable
effect size (MDES), and needed sample size for multi-level experimental
designs, in which units are nested within hierarchical structures such
as students nested within schools nested within school districts. The
statistical power is calculated for estimating the impact of a single
intervention on multiple outcomes. The package uses a frequentist
framework of mixed effects regression models, which is currently the
prevailing framework for estimating impacts from experiments in
education and other social policy research.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Other options include nonparametric or Bayesian methods,
but these are less prevalent in applied research (for example, see &lt;span class="citation"&gt;Gelman, Hill, and Yajima (2012)&lt;/span&gt;, &lt;span class="citation"&gt;Gelman, Hill, and Yajima (2007)&lt;/span&gt;).&lt;/p&gt;'><sup>1</sup></a></p>
<p>To our knowledge, none of the existing software tools for power
calculations allow researchers to account for multiple hypothesis tests
and the use of a multiple testing procedure (MTP). MTPs adjust
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
to reduce the likelihood of spurious findings when researchers are
testing for effects on multiple outcomes. This adjustment can result in
a substantial change in statistical power, greatly reducing the
probability of detecting effects when they do exist. Unfortunately, when
designing studies, researchers who plan to test for effects on multiple
outcomes and employ MTPs frequently ignore the power implications of the
MTPs.</p>
<p>Also, as researchers change their focus from one outcome to multiple
outcomes, multiple definitions of statistical power emerge (<span class="citation">Chen et al. (2011)</span>; <span class="citation">Dudoit, Shaffer, and Boldrick (2003)</span>; <span class="citation">Senn and Bretz (2007)</span>; <span class="citation">Westfall, Tobias, and Wolfinger (2011)</span>). The
<code>PUMP</code> package allows researchers to consider multiple
definitions of power, selecting those most suited to the goals of their
study. The definitions of power include:</p>
<ul>
<li>
<strong>individual power</strong>: the probability of detecting an
effect of a particular size (specified by the researcher) or larger for
each hypothesis test. Individual power corresponds to how power is
defined when there is focus on a single outcome.</li>
<li>
<strong><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
power</strong>: the probability of detecting effects of at least a
particular size on at least one outcome. Similarly, the researcher can
consider
<strong><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>−</mo></mrow><annotation encoding="application/x-tex">d-</annotation></semantics></math>minimal
power</strong> for any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
less than the number of outcomes, or fractional powers, such as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mn>2</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1/2-</annotation></semantics></math>minimal
power.</li>
<li>
<strong>complete power</strong>: the power to detect effects of at
least a particular size on <em>all</em> outcomes.</li>
</ul>
<p>As noted in <span class="citation">Porter (2018)</span>, the
prevailing default in many studies–-individual power–-may or may not be
the most appropriate type of power. If the researcher’s goal is to find
statistically significant estimates of effects on most or all primary
outcomes of interest, then their power may be much lower than
anticipated when multiplicity adjustments are taken into account. On the
other hand, if the researcher’s goal is to find statistically
significant estimates of effects on at least one or a small proportion
of outcomes, their power may be much better than anticipated. In both of
these cases, by not accounting for both the challenges and opportunities
arising from multiple outcomes, a researcher may find they have wasted
resources, either by designing an underpowered study that cannot detect
the desired effect sizes, or by designing an overpowered study that had
a larger sample size than necessary. We introduce the <code>PUMP</code>
package to allow for directly answering questions that take multiple
outcomes into account, such as:</p>
<ul>
<li>How many schools would I need to detect a given effect on at least
three of my five outcomes?</li>
<li>What size effect can I reliably detect on each outcome, given a
planned MTP across all my outcomes?</li>
<li>How would the power to detect a given effect change if only half my
outcomes truly had impact?</li>
</ul>
<p>The methods in the PUMP package build on those introduced in <span class="citation">Porter (2018)</span>. This earlier paper focused only
on a single RCT design and model — a multisite RCT with the blocked
randomization of individuals, in which effects are estimated using a
model with block-specific intercepts and with the assumption of constant
effects across all units. This earlier paper also did not produce
software to assist researchers in implementing its methods. With this
current paper and with the introduction of the PUMP package, we extend
the methodology to nine additional multi-level RCT designs and models.
Also, while <span class="citation">Porter (2018)</span> focused on
estimates of power, PUMP goes further to also estimate MDES and sample
size requirements that take multiplicity adjustments into account.</p>
<p><code>PUMP</code> extends functionality of the popular PowerUp!
<code>R</code> package (and its related tools in the form of a
spreadsheet and Shiny application), which compute power or MDES for
multi-level RCTs with a single outcome (<span class="citation">Dong and
Maynard (2013)</span>). For a wide variety of RCT designs with a single
outcome, researchers can take advantage of closed-form solutions and
numerous power estimation tools. For example, in education and social
policy research, see <span class="citation">Dong and Maynard
(2013)</span>; <span class="citation">Hedges and Rhoads (2010)</span>;
<span class="citation">Raudenbush et al. (2011)</span>. However,
closed-form solutions are difficult or impossible to derive when a MTP
is applied to a setting with multiple outcomes. Instead, we use a
simulation-based approach to achieve estimates of power.</p>
<p>In order to calculate power, the researcher specifies information
about the sample size at each level, the minimum detectable effect size
for each outcome, the level of statistical significance, and parameters
of the data generating distribution. The minimum detectable effect size
is the smallest true effect size the study can detect with the desired
statistical significance level, in units of standard deviations. An
“effect size” generally refers to the standardized mean difference
effect size, which “equals the difference in mean outcomes for the
treatment group and control group, divided by the standard deviation of
outcomes across subjects within experimental groups” (<span class="citation">Bloom (2006)</span>). Researchers often use effect
sizes to standardize outcomes so that outcomes with different scales can
be directly compared.</p>
<p>The package includes three core functions:</p>
<ul>
<li>
<code><a href="../reference/pump_power.html">pump_power()</a></code> for calculating power given a experimental
design and assumed model, parameters, and minimum detectable effect
size.</li>
<li>
<code><a href="../reference/pump_mdes.html">pump_mdes()</a></code> for calculating minimum detectable effect
size given a target power and sample sizes.</li>
<li>
<code><a href="../reference/pump_sample.html">pump_sample()</a></code> for calculating the required sample size
for achieving a given target power for a given minimum detectable effect
size.</li>
</ul>
<p>For any of these core functions, the user begins with two main
choices. First, the user chooses the assumed design and model of the
RCT. The <code>PUMP</code> package covers a range of multi-level
designs, up to three levels of hierarchy, that researchers typically use
in practice, in which research units are nested in hierarchical groups.
Our power calculations assume the user will be analyzing these RCTs
using frequentist mixed-effects regression models, containing a
combination of fixed or random intercepts and treatment impacts at
different levels. We explain these details in the accompanying paper, in
Section 4 and in the Technical Appendix. Second, the user chooses the
MTP to be applied. <code>PUMP</code> supports five common MTPs —
Bonferroni, Holm, single-step and step-down versions of Westfall-Young,
and Benjamini-Hochberg. After these two main choices, the user must also
make a variety of decisions about parameters of the data generating
distribution.</p>
<p>The package also includes functions that allow users to easily
explore power over a range of possible values of parameters. This
exploration encourages the user to determine the sensitivity of
estimates to different assumptions. <code>PUMP</code> also visually
displays results. These additional functions include:</p>
<ul>
<li>
<code><a href="../reference/pump_power_grid.html">pump_power_grid()</a></code>, <code><a href="../reference/pump_mdes_grid.html">pump_mdes_grid()</a></code>, and
<code><a href="../reference/pump_sample_grid.html">pump_sample_grid()</a></code> for calculating the given output over a
range of possible parameter values.</li>
<li>
<code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> to re-run an existing calculation with a small
number of parameters updated.</li>
<li>
<code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> on <code>PUMP</code>-generated objects to
generate plots (including grid outputs).</li>
</ul>
<p>The authors of the <code>PUMP</code> package have also created a web
application built with R Shiny. This web application calls the
<code>PUMP</code> package and allows users to conduct calculations with
a user-friendly interface, but it is less flexible than the package,
with a focus on simpler scenarios (e.g., 10 or fewer outcomes). The app
can be found at .</p>
</div>
<div class="section level2">
<h2 id="case-study-diplomas-now">Case study: Diplomas Now<a class="anchor" aria-label="anchor" href="#case-study-diplomas-now"></a>
</h2>
<p>We illustrate our package using an example of a published RCT that
evaluated a secondary school model called Diplomas Now. The Diplomas Now
model is designed to increase high school graduation rates and
post-secondary readiness. Evaluators conducted a RCT comparing schools
who implemented the model to business-as-usual. We refer to this example
throughout this paper to illustrate key concepts and to illustrate the
application of the <code>PUMP</code> package.</p>
<p>The Diplomas Now model, created by three national organizations,
Talent Development, City Year, and Communities In Schools, targets
underfunded urban middle and high schools with many students who are not
performing well academically. The model is designed to be robust enough
to transform high-poverty and high-needs middle and high schools
attended by many students who fall off the path to high school
graduation. Diplomas Now, with MDRC as a partner, was one of the first
validation grants awarded as part of the Investing in Innovation (i3)
competition administered by the federal Department of Education.</p>
<p>We follow the general design of the Diplomas Now evaluation,
conducted by MDRC. The RCT contains three levels (students within
schools within districts) with random assignment at level two (schools).
The initial evaluation, included two cohorts of schools with each cohort
implementing for two years (2011-2013 for Cohort 1 and 2012-2014 for
Cohort 2). The cohorts included 62 secondary schools (both middle and
high schools) in 11 school districts that agreed to participate. Schools
in the active treatment group were assigned to implement the Diplomas
Now model, while the schools in the control group continued their
existing school programs or implemented other reform strategies of their
choosing (<span class="citation">Corrin et al. (2016)</span>.) The MDRC
researchers conducted randomization of the schools within blocks defined
by district, school type, and year of roll-out. After some schools were
dropped from the study due to structural reasons, the researchers were
left with 29 high schools and 29 middle schools grouped in 21 random
assignment blocks. Within each block, schools were randomized to the
active treatment or business-as-usual, resulting in 32 schools in the
treatment group, and 30 schools in the control group.</p>
<p>The evaluation focused on three categories of outcomes: Attendance,
Behavior, and Course performance, called the “ABC’s”, with multiple
measures for each category. In addition, the evaluation measured an
overall ABC composite measures of whether a student is above given
thresholds on all three categories. This grouping constitutes 12 total
outcomes of interest. Evaluating each of the 12 outcomes independently
would not be good practice, as the chance of a spurious finding would
not be well controlled. The authors of the MDRC report pre-identified
three of these outcomes as <em>primary</em> outcomes before the start of
the study in order to reduce the problem of multiple testing. We, by
contrast, use this example to illustrate what could be done if there was
uncertainty as to which outcomes should be primary. In particular, we
illustrate how to conduct a power analysis to plan a study where one
uses multiple testing adjustment, rather than predesignation, to account
for the multiple outcome problem.</p>
<p>There are different guidelines for how to adjust for groupings of
multiple outcomes in education studies. For example, <span class="citation">Schochet (2008)</span> recommends organizing primary
outcomes into domains, conducting tests on composite domain outcomes,
and applying multiplicity corrections to composites across domains. The
What Works Clearinghouse applies multiplicity corrections to findings
within the same domain rather than across different domains. We do not
provide recommendations for which guidelines to follow when
investigating impacts on multiple outcomes. Rather, we address the fact
that researchers across many domains are increasingly applying MTPs and
therefore need to correctly estimate power, MDES and sample size
requirements accounting for this choice. In our example, we elect to do
a power analysis separately for each of the three outcome groups of the
ABC outcomes to control family-wise error rather than overall error.
This strategy means we adjust for the number of outcomes within each
group independently. For illustration purposes, we focus on one outcome
group, attendance, which we will assume contains five separate
outcomes.</p>
</div>
<div class="section level2">
<h2 id="user-choices">User choices<a class="anchor" aria-label="anchor" href="#user-choices"></a>
</h2>
<div class="section level3">
<h3 id="designs-and-models">Designs and models<a class="anchor" aria-label="anchor" href="#designs-and-models"></a>
</h3>
<p>When planning a study, the researcher first has to identify the
design of the experiment, including the number of levels, and the level
at which randomization occurs. These decisions can be a mix of the
realities of the context (e.g., the treatment must be applied at the
school level, and students are naturally nested in schools, making for a
cluster randomization), or deliberate (e.g., the researcher groups
similar schools to block their experiment in an attempt to improve
power). Second, based on the design and the inferential goals of the
study, the researchers chooses an assumed model, including whether
intercepts and treatment effects should be treated as constant, fixed,
or random. For the same experimental design, the analyst can sometimes
choose from a variety of possible models, and these two decisions should
be kept conceptually separated from each other.</p>
<p><em>The design.</em> The <code>PUMP</code> package supports designs
with one, two, or three levels, with randomization occurring at any
level. For example, a design with two levels and randomization at level
one is a blocked design (or equivalently a multisite experiment), where
level two forms the blocks (blocks being groups of units, some of which
are treated and some not). Ideally, the blocks in a trial will be groups
of relatively homogenous units, but frequently they are a consequence of
the units being studied (e.g., evaluations of college supports, with
students, the units, nested in colleges, the blocks). A design with two
levels and randomization at level two is commonly called a cluster
design (e.g., a collection of schools, with treatment applied to a
subset of the schools, with outcomes at the student level); here the
schools are the clusters, with a cluster being a collection of units
which is entirely treated or entirely not. We can also have both
blocking and clustering: randomizing schools within districts, creating
a series of cluster-randomized experiments, would be a blocked (by
district), cluster-randomized experiment, with randomization at level
two.</p>
<p><em>The model.</em> Given a design, the researcher can select a model
via a few modeling choices. In particular the researcher has to decide,
for each level beyond the first, about the intercepts and the treatment
impacts:</p>
<ul>
<li>Whether level two and level three intercepts are:
<ul>
<li>fixed: we have a separate intercept for each unit.</li>
<li>random: we have a separate intercept for each unit as above, but
model the collection of intercepts as Normally distributed, allowing for
partial pooling.</li>
</ul>
</li>
<li>Whether level two and level three treatment effects are:
<ul>
<li>constant: we model all units within a group as having the same
single average impact.</li>
<li>fixed: we allow each block or cluster within a level to have its own
individual estimated impact (we can only do this if we have treated and
control units within said block or cluster).</li>
<li>random: we allow variation as with fixed, but model the collection
of treatment impacts as Normally distributed around a grand mean mean
impact. This is implicitly allowing for the sample as being
representative of a larger super-population, in terms of treatment
impact estimation.</li>
</ul>
</li>
</ul>
<p>We denote the research design by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>,
followed by the number of levels and randomization level, so
<code>d3.1</code> is a three level design with randomization at level
one. The model is denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>,
followed by the level and the assumption for the intercepts, either
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>
and then the assumption for the treatment impacts,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>,
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>.
For example, <code>m3ff2rc</code> means at level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math>,
we assume fixed intercepts and fixed treatment impacts, and at level two
we assume random intercepts and constant treatment impacts. The full
design and model are specified by concatenating these together,
e.g. <code>d2.1_m3fc</code>. The Diplomas Now model, for example, is
<code>d3.2_m3fc2rc</code>.</p>
<p>The full list of supported design and model combinations is below.
The user can see the list by calling <code><a href="../reference/pump_info.html">pump_info()</a></code>, which
provides the designs and models, MTPs, power definitions, and model
parameters. We also include the corresponding names from the PowerUP!
package where appropriate. For more details about each combination of
design and model, see the Technical Appendix.</p>
</div>
<div class="section level3">
<h3 id="multiple-testing-procedures">Multiple testing procedures<a class="anchor" aria-label="anchor" href="#multiple-testing-procedures"></a>
</h3>
<p>Here we provide a review of the multiple testing procedures supported
by the <code>PUMP</code> package:</p>
<ul>
<li>
<em>Bonferroni</em>: adjusts
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
by multiplying them by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
to ensure strong control of the FWER. Bonferroni is a simple procedure,
but the most conservative.</li>
<li>
<em>Holm</em>: a step-down version of Bonferroni. Starting from
smallest to largest,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
are sequentially adjusted by different multipliers. Holm is less
conservative than Bonferroni for larger
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values.</li>
<li>
<em>Benjamini-Hochberg</em>: A sequential, step-up procedure that
controls the FDR. Using the BH method, only null hypotheses with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
below a certain threshold are rejected, where the threshold is
determined by the number of tests and the level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.</li>
<li>
<em>Single-step Westfall-Young</em>: A permutation-based procedure
for controlling the FWER, which directly takes into account the joint
correlation structure of the outcomes. In the single-step approach, all
outcomes are adjusted by using the permuted distribution of the minimum
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value.
Although Westfall-Young procedures are less conservative while still
protecting against false discoveries, they are computationally very
intensive.</li>
<li>
<em>Step-down Westfall-Young</em>: A similar approach to the
single-step procedure, except that outcomes are adjusted sequentially
from smallest to largest according to the permuted distributions of the
corresponding sequential
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values.</li>
</ul>
<p>For a more detailed explanation of each MTP, see Appendix A of <span class="citation">Porter (2018)</span>.</p>
<p>The following table from <span class="citation">Porter (2018)</span>
summarizes the important features for each of the MTPs supported by
<code>PUMP</code>.</p>
<table class="table">
<colgroup>
<col width="27%">
<col width="17%">
<col width="27%">
<col width="27%">
</colgroup>
<thead><tr class="header">
<th>Procedure</th>
<th>Control</th>
<th>Single-step or stepwise</th>
<th>Accounts for correlation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Bonferroni (BF)</td>
<td>FWER</td>
<td>single-step</td>
<td>No</td>
</tr>
<tr class="even">
<td>Holm (HO)</td>
<td>FWER</td>
<td>stepwise</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Westfall-Young Single-step (WY-SS)</td>
<td>FWER</td>
<td>single-step</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Westfall-Young Step-down (WY-SD)</td>
<td>FWER</td>
<td>stepwise</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Benjamini-Hochberg (BH)</td>
<td>FDR</td>
<td>stepwise</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="model-parameters">Model parameters<a class="anchor" aria-label="anchor" href="#model-parameters"></a>
</h3>
<p>The table below shows the parameters that influence
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mi>m</mi></msub><annotation encoding="application/x-tex">Q_m</annotation></semantics></math>,
the standard error, for different designs and models.</p>
<p>A few parameters warrant more explanation.</p>
<ul>
<li><p>The quantity
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="normal">ICC</mtext><annotation encoding="application/x-tex">\text{ICC}</annotation></semantics></math>
is the unconditional Intraclass Correlation, and gives a measure of
variation at different levels of the model. For each outcome, the ICC
for each level is defined as the ratio of the variance at that level
divided by the overall variance of the individual outcomes. The ICC
includes the variation due to covariates.</p></li>
<li><p>For each outcome, the quantity omega
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>)
for each level is the ratio between impact variation at that level and
variation in intercepts (including covariates) at that level. It is a
measure of treatment impact heterogeneity.</p></li>
<li><p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math>
expressions are the percent of variation at a particular level predicted
by covariates specific to that level. For simplicity we assume
covariates at a level are group mean centered, so only covariates at a
particular level explain variance at that level.</p></li>
</ul>
<p>For precise formulae of these expressions, see the Technical
Appendix, which outlines the assumed data-generating process, and the
resulting expressions for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="normal">ICC</mtext><annotation encoding="application/x-tex">\text{ICC}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math>.</p>
<p>In addition to design parameters, there are additional parameters
that control the precision of the power estimates themselves:</p>
<ul>
<li>
<code>tnum</code> is the number of test statistics generated in
order to estimate power. A larger number of test statistics results in
greater computation time, but also a more precise estimate of power.
Note that the <code><a href="../reference/pump_mdes.html">pump_mdes()</a></code> and <code><a href="../reference/pump_sample.html">pump_sample()</a></code>
have multiple <code>tnum</code> parameters controlling the precision of
the search.</li>
<li>
<code>B</code> is the number of Westfall-Young permutations. Again,
there is a tradeoff between precision and computation time.</li>
<li>
<code>parallel.WY.cores</code> specifies the number of cores to use
for parallel computation of the Westfall-Young Step-Down procedure,
which is the most computationally intensive. The default of
<code>1</code> does not result in parallel computation. Parallelization
is done using <code>parApply</code> from the <code>parallel</code>
package.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="package-illustration">Package illustration<a class="anchor" aria-label="anchor" href="#package-illustration"></a>
</h2>
<p>In this section, we illustrate how to use the <code>PUMP</code>
package, using our example motivated by the Diplomas Now study. Given
the study’s design, we ask a natural initial question: What size of
impact could we reasonably detect after using a MTP to adjust
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
to account for our multiple outcomes?</p>
<p>We mimic the planning process one might use for planning a study
similar to Diplomas Now (e.g., if we were planning a replication trial
in a slightly different context). To answer this question we therefore
first have to decide on our experimental design and modeling approach.
We also have to determine values for the associated design parameters
that accompany these choices. In the following sections we walk through
selecting these parameters (sample size, control variables, intraclass
correlation coefficients, impact variation, and correlation of
outcomes). We calculate MDES for the resulting context and determine how
necessary sample sizes change depending on what kind of power we desire.
We finally illustrate some sensitivity checks, looking at how MDES
changes as a function of rho, the correlation of the test
statistics.</p>
<div class="section level3">
<h3 id="establishing-needed-design-parameters">Establishing needed design parameters<a class="anchor" aria-label="anchor" href="#establishing-needed-design-parameters"></a>
</h3>
<p>To conduct power, MDES, and sample size calculations, we first
specify the design, sample sizes, analytic model, and level of
statistical significance. We also must specify parameters of the data
generating distribution that match the selected design and model. All of
these numbers have to be determined given resource limitations, or
estimated using prior knowledge, pilot studies, or other sources of
information.</p>
<p>We next discuss selection of all needed design parameters and
modeling choices. For further discussion of selecting these parameters
see, for example <span class="citation">Bloom (2006)</span> and <span class="citation">Dong and Maynard (2013)</span>. For discussion in the
multiple testing context, especially with regards to the overall power
measures such as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
or complete power, see <span class="citation">Porter (2018)</span>; the
findings there are general, as they are a function of the final
distribution of test statistics. The key insight is that power is a
function of only a few summarizing elements: the individual-level
standard errors, the degrees of freedom, and the correlation structure
of the test statistics. Once we have these elements, regardless of the
design, we can proceed.</p>
<p><em>Analytic model.</em> We first need to specify how we will analyze
our data; this choice also determines which design parameters we will
need to specify. Following the original Diplomas Now report, we plan on
using a multi-level model with fixed effects at level three, a random
intercept at level two, and a single treatment coefficient. We represent
this model as “m3fc2rc.” The “3fc” means we are including block fixed
effects, and not modeling any treatment impact variation at level three.
The “2rc” means random intercept and no modeled variation of treatment
within each block (the “c” is for “constant”). We note that the Diplomas
Now report authors call their model a “two-level” model, but this is not
quite aligned with the language of this package. In particular, fixed
effects included at level two are actually accounting for variation at
level three; we therefore identify their model as a three level model
with fixed effects at level three.</p>
<p><em>Sample sizes.</em> We assume equal size randomization blocks and
schools, as is typical of most power analysis packages. For our context,
this gives about three schools per randomization block; we can later do
a sensitivity check where we increase and decrease this to see how power
changes. The Diplomas Now report states there were 14,950 students,
yielding around 258 students per school. Normally we would use the
geometric means of schools per randomization block and students per
school as our design parameters, but that information is not available
in the report. We assume 50% of the schools are treated; our
calculations will be approximate here in that we could not actually
treat exactly 50% in small and odd-sized blocks.</p>
<p><em>Control variables.</em> We next need values for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math>
of the possible covariates. The report does not provide these
quantities, but it does mention covariate adjustment in the presentation
of the model. Given the types of outcomes we are working with, it is
unlikely that there are highly predictive individual-level covariates,
but our prior year school-average attendance measures are likely to be
highly predictive of corresponding school-average outcomes. We thus set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>R</mi><mn>1</mn><mn>2</mn></msubsup><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">R^2_1 = 0.1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>R</mi><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">R^2_2 = 0.5</annotation></semantics></math>.
We assume five covariates at level one and three at level two; this
decision, especially for level one, usually does not matter much in
practice, unless sample sizes are very small (the number of covariates
along with sample size determine the degrees of freedom for our planned
tests).</p>
<p><em>ICCs.</em> We also need a measure of where variation occurs: the
individual, the school, or the randomization block level. We capture
this with Intraclass Correlation Coefficients (ICCs), one for level two
and one for level three. ICC measures specify overall variation in
outcome across levels: e.g., do we see relatively homogeneous students
within schools that are quite different, or are the schools generally
the same with substantial variation within them? We typically would
obtain ICCs from pilot data or external reports on similar data. We here
specify a level-two ICC of 0.05, and a level-three ICC of 0.40. We set a
relatively high level three ICC as we expect our school type by district
blocks to isolate variation; in particular we might believe middle and
high school attendance rates would be markedly different.</p>
<p><em>Correlation of outcomes.</em> We finally need to specify the
number and relationship among our outcomes and associated
test-statistics. For illustration, we select attendance as our outcome
group. We assume we have five different attendance measures. The main
decision regarding outcomes is the correlation of our test statistics.
As a rough proxy, we use the correlation of the outcomes at the level of
randomization; in our case this would be the correlation of
school-average attendance within block. We believe the attendance
measures would be fairly related, so we select <code>rho = 0.40</code>
for all pairs of outcomes. This value is an estimate, and we strongly
encourage exploration of different values of this correlation choice as
a sensitivity check for any conducted analysis. Selecting a candidate
rho is difficult, and will be new for those only familiar with power
analyses of single outcomes; we need to more research in the field, both
empirical and theoretical, to further guide this choice.</p>
<p>If the information were available, we could specify different values
for the design parameters such as the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math>s
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>C</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">ICC</annotation></semantics></math>s
for each outcome, if we thought they had different characteristics; for
simplicity we do not do this here. The <code>PUMP</code> package also
allows specifying different pairwise correlations between the test
statistics of the different outcomes via a matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>s
rather than a single
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>;
also for simplicity, we do not do that here.</p>
<p>Once we have established initial values for all needed parameters, we
first conduct a baseline calculation, and then explore how MDES or other
quantities change as these parameters change.</p>
</div>
<div class="section level3">
<h3 id="calculating-mdes">Calculating MDES<a class="anchor" aria-label="anchor" href="#calculating-mdes"></a>
</h3>
<p>We now have an initial planned design, with a set number of schools
and students. But is this a large enough experiment to reliably detect
reasonably sized effects? To answer this question we calculate the
minimal detectable effect size (MDES), given our planned analytic
strategy, for our outcomes.</p>
<p>To identify the MDES of a given setting we use the
<code>pump_mdes</code> method, which conducts a search for a MDES that
achieves a target level of power. The MDES depends on all the design and
model parameters discussed above, but also depends on the type of power
and target level of power we are interested in. For example, we could
determine what size effect we can reliably detect on our first outcome,
after multiplicity adjustment. Or, we could determine what size effects
we would need across our five outcomes to reliably detect an impact on
at least one of them. We set our goal by specifying the type
(<code>power.definition</code>) and desired power
(<code>target.power</code>).</p>
<p>Here, for example, we find the MDES if we want an 80% chance of
detecting an impact on our first outcome when using the Holm
procedure:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pump_mdes.html">pump_mdes</a></span><span class="op">(</span></span>
<span>  d_m <span class="op">=</span> <span class="st">"d3.2_m3fc2rc"</span>,         <span class="co"># choice of design and analysis strategy</span></span>
<span>  MTP <span class="op">=</span> <span class="st">"HO"</span>,                   <span class="co"># multiple testing procedure</span></span>
<span>  target.power <span class="op">=</span> <span class="fl">0.80</span>,          <span class="co"># desired power</span></span>
<span>  power.definition <span class="op">=</span> <span class="st">"D1indiv"</span>, <span class="co"># power type</span></span>
<span>  M <span class="op">=</span> <span class="fl">5</span>,                        <span class="co"># number of outcomes</span></span>
<span>  J <span class="op">=</span> <span class="fl">3</span>,                        <span class="co"># number of schools per block</span></span>
<span>  K <span class="op">=</span> <span class="fl">21</span>,                       <span class="co"># number districts</span></span>
<span>  nbar <span class="op">=</span> <span class="fl">258</span>,                   <span class="co"># average number of students per school</span></span>
<span>  Tbar <span class="op">=</span> <span class="fl">0.50</span>,                  <span class="co"># prop treated</span></span>
<span>  alpha <span class="op">=</span> <span class="fl">0.05</span>,                 <span class="co"># significance level</span></span>
<span>  numCovar.1 <span class="op">=</span> <span class="fl">5</span>,               <span class="co"># number of covariates at level 1</span></span>
<span>  numCovar.2 <span class="op">=</span> <span class="fl">3</span>,               <span class="co"># number of covariates at level 2</span></span>
<span>  R2.1 <span class="op">=</span> <span class="fl">0.1</span>, R2.2 <span class="op">=</span> <span class="fl">0.7</span>,       <span class="co"># explanatory power of covariates for each level</span></span>
<span>  ICC.2 <span class="op">=</span> <span class="fl">0.05</span>, ICC.3 <span class="op">=</span> <span class="fl">0.4</span>,    <span class="co"># intraclass correlation coefficients</span></span>
<span>  rho <span class="op">=</span> <span class="fl">0.4</span> <span class="op">)</span>                   <span class="co"># how correlated outcomes are</span></span></code></pre></div>
<p>The results are easily made into a nice table via the
<code>knitr</code> <code>kable()</code> command:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/kable.html" class="external-link">kable</a></span><span class="op">(</span> <span class="va">m</span>, digits <span class="op">=</span> <span class="fl">3</span> <span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">kableExtra</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/kableExtra/man/kable_styling.html" class="external-link">kable_styling</a></span><span class="op">(</span> position <span class="op">=</span> <span class="st">"center"</span> <span class="op">)</span></span></code></pre></div>
<table class="table table" style="margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:left;">
MTP
</th>
<th style="text-align:right;">
Adjusted.MDES
</th>
<th style="text-align:right;">
D1indiv.power
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;">
HO
</td>
<td style="text-align:right;">
0.105
</td>
<td style="text-align:right;">
0.807
</td>
</tr></tbody>
</table>
<p>The answers <code><a href="../reference/pump_mdes.html">pump_mdes()</a></code> gives are approximate as we are
calculating them via monte carlo simulation. To control accuracy, we can
specify a tolerance (<code>tol</code>) of how close the estimated power
needs to be to the desired target along with the number of iterations in
the search sequence (via <code>start.tnum</code>, <code>tnum</code>, and
<code>final.tnum</code>). The search will stop when the estimated power
is within <code>tol</code> of <code>target.power</code>, as estimated by
<code>final.tnum</code> iterations. Lower tolerance and higher
<code>tnum</code> values will give more exact results (and take more
computational time).</p>
<p>Changing the type of power is straightforward: for example, to
identify the MDES for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
power (i.e., what effect do we have to assume across all observations
such that we will find at least one significant result with 80% power?),
we simply update our result with our new power definition:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">m</span>, power.definition <span class="op">=</span> <span class="st">"min1"</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; mdes result: d3.2_m3fc2rc d_m with 5 outcomes</span></span>
<span><span class="co">#&gt;   target min1 power: 0.80</span></span>
<span><span class="co">#&gt;  MTP Adjusted.MDES min1.power   SE</span></span>
<span><span class="co">#&gt;   HO    0.08048574    0.78425 0.01</span></span>
<span><span class="co">#&gt;  (5 steps in search)</span></span></code></pre>
<p>The <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> method can replace any number of arguments
of the prior call with new ones, making exploration of different
scenarios very straightforward.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The update() method re-runs the underlying call of
pump_mdes(), pump_sample(), or pump_power() with the revised set of
design parameters. You can even change which call to use via the type
parameter.&lt;/p&gt;"><sup>2</sup></a> Our results show that if we just want to
detect at least one outcome with 80% power, we can reliably detect an
effect of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.08</mn><annotation encoding="application/x-tex">0.08</annotation></semantics></math>
(assuming all five outcomes have effects of at least that size).</p>
<p>When estimating power for multiple outcomes, it is important to
consider cases where some of the outcomes in fact have null, or very
small, effects, to hedge against circumstances such as one of the
outcomes not being well measured. One way to do this is to set two of
our outcomes to no effect with the <code>numZero</code> parameter:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">m2</span>, numZero <span class="op">=</span> <span class="fl">2</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; mdes result: d3.2_m3fc2rc d_m with 5 outcomes</span></span>
<span><span class="co">#&gt;   target min1 power: 0.80</span></span>
<span><span class="co">#&gt;  MTP Adjusted.MDES min1.power   SE</span></span>
<span><span class="co">#&gt;   HO    0.08970923    0.79125 0.01</span></span>
<span><span class="co">#&gt;  (13 steps in search)</span></span></code></pre>
<p>The MDES goes up, as expected: when there are not effects on some
outcomes, there are fewer good chances for detecting an effect.
Therefore, an increased MDES (for the nonzero outcomes) is required to
achieve the same level of desired power (80%). Below we provide a deeper
dive into the extent to which <code>numZero</code> can effect power
estimates.</p>
</div>
<div class="section level3">
<h3 id="determining-necessary-sample-size">Determining necessary sample size<a class="anchor" aria-label="anchor" href="#determining-necessary-sample-size"></a>
</h3>
<p>The MDES calculator tells us what we can detect given a specific
design. We might instead want to ask how much larger our design would
need to be in order to achieve a desired MDES. In particular, we might
want to determine the needed number of students per school, the number
of schools, or the number of blocks to detect an effect of a given size.
The <code>pump_sample</code> method will search over any one of
these.</p>
<p>Assuming we have three schools per block, we first calculate how many
blocks we would need to achieve a MDES of 0.10 for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
power (this answers the question of how big of an experiment do we need
in order to have an 80% chance of finding at least one outcome
significant, if all outcomes had a true effect size of 0.10):</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">smp</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pump_sample.html">pump_sample</a></span><span class="op">(</span></span>
<span>  d_m <span class="op">=</span> <span class="st">"d3.2_m3fc2rc"</span>,</span>
<span>  MTP <span class="op">=</span> <span class="st">"HO"</span>,</span>
<span>  typesample <span class="op">=</span> <span class="st">"K"</span>,</span>
<span>  target.power <span class="op">=</span> <span class="fl">0.80</span>, power.definition <span class="op">=</span> <span class="st">"min1"</span>, tol <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span>  MDES <span class="op">=</span> <span class="fl">0.10</span>, M <span class="op">=</span> <span class="fl">5</span>, nbar <span class="op">=</span> <span class="fl">258</span>, J <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  Tbar <span class="op">=</span> <span class="fl">0.50</span>, alpha <span class="op">=</span> <span class="fl">0.05</span>, numCovar.1 <span class="op">=</span> <span class="fl">5</span>, numCovar.2 <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  R2.1 <span class="op">=</span> <span class="fl">0.1</span>, R2.2 <span class="op">=</span> <span class="fl">0.7</span>, ICC.2 <span class="op">=</span> <span class="fl">0.05</span>, ICC.3 <span class="op">=</span> <span class="fl">0.40</span>, rho <span class="op">=</span> <span class="fl">0.4</span> <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span> <span class="va">smp</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; sample result: d3.2_m3fc2rc d_m with 5 outcomes</span></span>
<span><span class="co">#&gt;   target min1 power: 0.80</span></span>
<span><span class="co">#&gt;  MTP Sample.type Sample.size min1.power   SE</span></span>
<span><span class="co">#&gt;   HO           K          16      0.797 0.01</span></span>
<span><span class="co">#&gt;  (18 steps in search)</span></span></code></pre>
<p>We would need 15 blocks, rather than the originally specified 21,
giving 45 total schools in our study, to achieve 80%
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
power.</p>
<p>We recommend checking MDES and sample-size calculators, as the
estimation error combined with the stochastic search can give results a
bit off the target in some cases. A check is easy to do; simply run the
found design through <code><a href="../reference/pump_power.html">pump_power()</a></code>, which directly
calculates power for a given scenario, to see if we recover our
originally targeted power (we can use <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> and set the
type to <code>power</code> to pass all the design parameters
automatically). When we do this, we can also increase the number of
iterations to get more precise estimates of power, as well:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_check</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">smp</span>, type <span class="op">=</span> <span class="st">"power"</span>, tnum <span class="op">=</span> <span class="fl">20000</span>,</span>
<span>                   long.table <span class="op">=</span> <span class="cn">TRUE</span> <span class="op">)</span></span></code></pre></div>
<table class="table table" style="margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:left;">
power
</th>
<th style="text-align:right;">
None
</th>
<th style="text-align:right;">
HO
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
individual outcome 1
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:left;">
individual outcome 2
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:left;">
individual outcome 3
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:left;">
individual outcome 4
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:left;">
individual outcome 5
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:left;">
mean individual
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:left;">
1-minimum
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.84
</td>
</tr>
<tr>
<td style="text-align:left;">
2-minimum
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.69
</td>
</tr>
<tr>
<td style="text-align:left;">
3-minimum
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.56
</td>
</tr>
<tr>
<td style="text-align:left;">
4-minimum
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.44
</td>
</tr>
<tr>
<td style="text-align:left;">
complete
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.37
</td>
</tr>
</tbody>
</table>
<p>When calculating power directly, we get power for all the implemented
definitions of power applicable to the design.</p>
<p>In the above, the first five rows are the powers for rejecting each
of the five outcomes—they are (up to simulation error) the same since we
are assuming the same MDES and other design parameters for each. The
“mean individual” is the mean individual power across all outcomes. The
first column is power without adjustment, and the second has our power
with the listed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value
adjustment.</p>
<p>The next rows show different multi-outcome definitions of power. In
particular, <code>1-minimum</code> shows the chance of rejecting at
least one hypotheses. The <code>complete</code> row shows the power to
reject all hypotheses; it is only defined if all outcomes are specified
to have a non-zero effect.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The package does not show power for these without
adjustment for multiple testing, as that power would be grossly inflated
and meaningless.&lt;/p&gt;"><sup>3</sup></a></p>
<p>We can look at a power curve of our <code><a href="../reference/pump_sample.html">pump_sample()</a></code> call
to assess how sensitive power is to our level two sample size:<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The points on the plots show the evaluated simulation
trials, with larger points corresponding to more iterations and greater
precision.&lt;/p&gt;"><sup>4</sup></a></p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span> <span class="va">smp</span> <span class="op">)</span></span></code></pre></div>
<p><img src="pump_demo_files/figure-html/plotsamplepowercurve-1.png" width="480" style="display: block; margin: auto;"></p>
<p>Though increasing <code>tnum</code> is useful for checking the power
calculation, it also increases computation time. Thus, for future
calculations we save a call with the default <code>tnum</code> to reduce
computation time.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pow</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">p_check</span>, tnum <span class="op">=</span> <span class="fl">10000</span> <span class="op">)</span></span></code></pre></div>
<p><em>Remark.</em> In certain settings, a wide range of sample sizes
may result in very similar levels of power. In this case, the algorithm
may return a sample size that is larger than necessary. This pattern
does not occur for the sample size at the highest level of the
hierarchy, and only for occurs for sample sizes at lower levels of the
hierarchy; e.g. for <code>nbar</code> for all models, and for
<code>nbar</code> and <code>J</code> for three level models. In
addition, due to the nature of the search algorithm, occasionally the
algorithm may not converge. For a more detailed discussion of these
challenges, see the package sample size vignette.</p>
</div>
<div class="section level3">
<h3 id="comparing-adjustment-procedures">Comparing adjustment procedures<a class="anchor" aria-label="anchor" href="#comparing-adjustment-procedures"></a>
</h3>
<p>It is easy to rerun the above using the Westfall-Young Stepdown
procedure (this procedure is much more computationally intensive to
run), or other procedures of interest. Alternatively, simply provide a
list of procedures you wish to compare. If you provide a list, the
package will re-run the power calculator for each item on the list; this
can make the overall call computationally intensive. Here we obtain
power for our scenario using Bonferroni, Holm and Westfall-Young
adjustments, and plot the results using the default <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code>
method:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">pow</span>,</span>
<span>              MTP <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span> <span class="st">"BF"</span>, <span class="st">"HO"</span>, <span class="st">"WY-SD"</span> <span class="op">)</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span> <span class="va">p2</span> <span class="op">)</span></span></code></pre></div>
<p><img src="pump_demo_files/figure-html/othercorrections-compile-1.png" width="672" style="display: block; margin: auto;"></p>
<p>To speed up computation, we could add
<code>parallel.WY.cores = 2</code> (or however many cores we wish to
allocate) to the call to parallelize the computation. We could also
reduce <code>tnum</code> to decrease computation time.</p>
<p>The more sophisticated (and less conservative) adjustment exploits
the correlation in our outcomes (<code>rho = 0.4</code>) to provide
higher individual power. Note, however, that we do not see elevated
rates for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
power. Accounting for the correlation of the test statistics when
adjusting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
can drive some power (individual power) up, but on the flip side
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>power
can be driven down as the lack of independence between tests gives fewer
chances for a significant result. See <span class="citation">Porter
(2018)</span> for further discussion; while the paper focuses on the
multisite randomized trial context, the lessons learned there apply to
all designs as the only substantive differences between different design
and modeling choices is in how we calculate the unadjusted distribution
of their test statistics.</p>
</div>
<div class="section level3">
<h3 id="exploring-sensitivity-to-design-parameters">Exploring sensitivity to design parameters<a class="anchor" aria-label="anchor" href="#exploring-sensitivity-to-design-parameters"></a>
</h3>
<p>Within the pump package we have two general ways of exploring design
sensitivity. The first is with <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code>, which allows for
quickly generating a single alternate scenario. To explore sensitivity
to different design parameters more systematically, use the
<code><a href="https://rdrr.io/r/graphics/grid.html" class="external-link">grid()</a></code> functions, which calculate power, mdes, and sample
size for all combinations of a set of passed parameter values. There are
two main differences between the two approaches. First,
<code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> allows for different values of a parameter for the
different outcomes; the <code>grid</code> approach, by contrast, is more
limited in this regard, and assumes the same parameter value across
different outcomes. Second, the <code>grid</code> functions are a
powerful tool for systematically exploring many possible combinations,
while <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> only allows the user to explore one value at
a time.</p>
<p>We first illustrate the <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> approach, and then turn
to illustrating <code><a href="https://rdrr.io/r/graphics/grid.html" class="external-link">grid()</a></code> across three common areas of
exploration: Intraclass Correlation Coefficients (ICCs), the correlation
of test statistics, and the assumed number of non-zero effects. The last
two are particularly important for multiple outcome contexts.</p>
<div class="section level4">
<h4 id="exploring-power-with-update">Exploring power with update()<a class="anchor" aria-label="anchor" href="#exploring-power-with-update"></a>
</h4>
<p>Update allows for a quick change of some of the set of parameters
used in a prior call; we saw <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> used several times
above. As a further example, here we examine what happens if the ICCs
are more equally split across levels two and three:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">pow</span>, ICC.2 <span class="op">=</span> <span class="fl">0.20</span>, ICC.3 <span class="op">=</span> <span class="fl">0.25</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span> <span class="va">p_b</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; power result: d3.2_m3fc2rc d_m with 5 outcomes</span></span>
<span><span class="co">#&gt;   MTP    D1indiv    D2indiv    D3indiv    D4indiv    D5indiv indiv.mean  min1</span></span>
<span><span class="co">#&gt;  None      0.261      0.257      0.268      0.262      0.263      0.262      </span></span>
<span><span class="co">#&gt;    SE ( 0.072 )  ( 0.072 )  ( 0.072 )  ( 0.072 )  ( 0.072 )                  </span></span>
<span><span class="co">#&gt;    HO      0.108      0.109      0.110      0.110      0.112      0.110 0.294</span></span>
<span><span class="co">#&gt;   min2  min3  min4 complete df1</span></span>
<span><span class="co">#&gt;                              29</span></span>
<span><span class="co">#&gt;                                </span></span>
<span><span class="co">#&gt;  0.133 0.067 0.037    0.028    </span></span>
<span><span class="co">#&gt;  0.000 &lt;= MCSE &lt;= 0.002</span></span></code></pre>
<p>We immediately see that our assumption of substantial variation in
level three matters a great deal for power.</p>
<p>When calculating power for a given scenario, it is also easy to vary
many of our design parameters by outcome. For example, if we thought we
had better predictive covariates for our second outcome, we might
try:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span> <span class="va">pow</span>,</span>
<span>               R2.1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span> <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span> <span class="op">)</span>,</span>
<span>               R2.2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span> <span class="fl">0.4</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span> <span class="op">)</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span> <span class="va">p_d</span> <span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; power result: d3.2_m3fc2rc d_m with 5 outcomes</span></span>
<span><span class="co">#&gt;   MTP    D1indiv    D2indiv    D3indiv    D4indiv    D5indiv indiv.mean  min1</span></span>
<span><span class="co">#&gt;  None      0.460      0.880      0.411      0.365      0.368      0.497      </span></span>
<span><span class="co">#&gt;    SE ( 0.052 )  ( 0.031 )  ( 0.055 )  ( 0.059 )  ( 0.059 )                  </span></span>
<span><span class="co">#&gt;    HO      0.269      0.701      0.237      0.207      0.207      0.324 0.754</span></span>
<span><span class="co">#&gt;   min2  min3  min4 complete df1</span></span>
<span><span class="co">#&gt;                              29</span></span>
<span><span class="co">#&gt;                                </span></span>
<span><span class="co">#&gt;  0.413 0.238 0.138    0.100    </span></span>
<span><span class="co">#&gt;  0.001 &lt;= MCSE &lt;= 0.002</span></span></code></pre>
<p>Notice how the individual powers are heavily impacted. The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-minimal
powers naturally take the varying outcomes into account as we are
calculating a joint distribution of test statistics that will have the
correct marginal distributions based on these different design parameter
values.</p>
<p>After several <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code>s, we may lose track of where we
are; to find out, we can always check details with
<code><a href="../reference/print_context.html">print_context()</a></code> or <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code>:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span> <span class="va">p_d</span> <span class="op">)</span></span>
<span><span class="co">#&gt; power result: d3.2_m3fc2rc d_m with 5 outcomes</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   MDES vector: 0.1, 0.1, 0.1, 0.1, 0.1</span></span>
<span><span class="co">#&gt;   nbar: 258  J: 3    K: 16   Tbar: 0.5</span></span>
<span><span class="co">#&gt;   alpha: 0.05    </span></span>
<span><span class="co">#&gt;   Level:</span></span>
<span><span class="co">#&gt;     1: R2: 0.1 / 0.3 / 0.1 / 0.2 / 0.2 (5 covariates)</span></span>
<span><span class="co">#&gt;     2: R2: 0.4 / 0.8 / 0.3 / 0.2 / 0.2 (3 covariates)    ICC: 0.05   omega: 0</span></span>
<span><span class="co">#&gt;     3:   fixed effects   ICC: 0.4    omega: 0</span></span>
<span><span class="co">#&gt;   rho = 0.4</span></span>
<span><span class="co">#&gt;   MTP    D1indiv    D2indiv    D3indiv    D4indiv    D5indiv indiv.mean  min1</span></span>
<span><span class="co">#&gt;  None      0.460      0.880      0.411      0.365      0.368      0.497      </span></span>
<span><span class="co">#&gt;    SE ( 0.052 )  ( 0.031 )  ( 0.055 )  ( 0.059 )  ( 0.059 )                  </span></span>
<span><span class="co">#&gt;    HO      0.269      0.701      0.237      0.207      0.207      0.324 0.754</span></span>
<span><span class="co">#&gt;   min2  min3  min4 complete df1</span></span>
<span><span class="co">#&gt;                              29</span></span>
<span><span class="co">#&gt;                                </span></span>
<span><span class="co">#&gt;  0.413 0.238 0.138    0.100    </span></span>
<span><span class="co">#&gt;  0.001 &lt;= MCSE &lt;= 0.002</span></span>
<span><span class="co">#&gt;  (tnum = 10000)</span></span></code></pre></div>
<p>Using update allows for targeted comparison of major choices, but if
we are interested in how power changes across a range of options, we can
do this more systematically with the <code><a href="https://rdrr.io/r/graphics/grid.html" class="external-link">grid()</a></code> functions, as
we do next.</p>
</div>
<div class="section level4">
<h4 id="exploring-the-impact-of-the-icc">Exploring the impact of the ICC<a class="anchor" aria-label="anchor" href="#exploring-the-impact-of-the-icc"></a>
</h4>
<p>We above saw that the ICC does impact power considerably. We next
extend this evaluation by exploring a range of options for both level
two and three ICCs, so we can assess whether our power is sufficient
across a set of plausible values. The <code><a href="../reference/update_grid.html">update_grid()</a></code> call
makes this straightforward: we pass our baseline scenario along with
lists of parameters to additionally explore. To decrease computation
time and achieve less precise estimates in this exploratory phase, we
decrease <code>tnum</code>. We can then easily visualize the variation
in min1 power by calling <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> on the object.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/update_grid.html">update_grid</a></span><span class="op">(</span> <span class="va">pow</span>,</span>
<span>            ICC.2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">0.3</span>, <span class="fl">0.05</span> <span class="op">)</span>,</span>
<span>            ICC.3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">0.60</span>, <span class="fl">0.20</span> <span class="op">)</span> <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span> <span class="va">grid</span>, power.definition <span class="op">=</span> <span class="st">"min1"</span> <span class="op">)</span></span></code></pre></div>
<p><img src="pump_demo_files/figure-html/ICCgrid-compile-1.png" width="672" style="display: block; margin: auto;"></p>
<p>Our plots show the impact of one varying factor averaged over the
other factors, like you would get with a plot for a main effect in a
fully interacted experiment. Note that in addition to
<code><a href="../reference/update_grid.html">update_grid()</a></code>, there are also base functions
<code><a href="../reference/pump_power_grid.html">pump_power_grid()</a></code>, <code><a href="../reference/pump_mdes_grid.html">pump_mdes_grid()</a></code>, and
<code><a href="../reference/pump_sample_grid.html">pump_sample_grid()</a></code>.</p>
<p>We see that higher ICC.2 radically reduces power to detect anything
and ICC.3 does little. To understand why, we turn to our standard error
formula for this design and model:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>S</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>τ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mrow><mfrac><mrow><msub><mtext mathvariant="normal">ICC</mtext><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msubsup><mi>R</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mover><mi>T</mi><mo accent="true">‾</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mover><mi>T</mi><mo accent="true">‾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>J</mi><mi>K</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mtext mathvariant="normal">ICC</mtext><mn>2</mn></msub><mo>−</mo><msub><mtext mathvariant="normal">ICC</mtext><mn>3</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msubsup><mi>R</mi><mn>1</mn><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mover><mi>T</mi><mo accent="true">‾</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mover><mi>T</mi><mo accent="true">‾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>J</mi><mi>K</mi><mover><mi>n</mi><mo accent="true">‾</mo></mover></mrow></mfrac></mrow></msqrt><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
SE( \hat{\tau} ) = \sqrt{
\frac{\text{ICC}_{2}(1 - R^2_{2})}{\bar{T}(1 - \bar{T}) JK} +
\frac{(1-\text{ICC}_{2} - \text{ICC}_{3})(1-R^2_{1})}{\bar{T}(1 - \bar{T}) J K\bar{n}} } .
\end{aligned}
</annotation></semantics></math> In the above, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>n</mi><mo accent="true">‾</mo></mover><mo>=</mo><mn>258</mn></mrow><annotation encoding="application/x-tex">\bar{n} = 258</annotation></semantics></math>
students per group makes the second term very small compared to the
first, regardless of the ICC.3 value. The first term, however, is a
direct scaling of ICC.2; changing it will change the standard error, and
therefore power, a lot. All provided designs and models implemented in
the package are discussed, along with corresponding formula such as
these, in our technical supplement accompanying this paper and
package.</p>
<p>For grid searches we recommend reducing the number of permutations,
via <code>tnum</code>, to speed up computation. As <code>tnum</code>
shrinks, we will get increasingly rough estimates of power, but even
these rough estimates can help us determine trends.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/grid.html" class="external-link">grid()</a></code> functions provide easy and direct ways of
exploring how power changes as a function of the design parameters. We
note, however, that in order to keep syntax simple, they do not allow
different design parameters, including MDES, by outcome. This is to keep
package syntax simpler. When faced with contexts where it is believed
that these parameters do vary, we recommend using average values for the
broader searches, and then double-checking a small set of potential
final designs with the <code><a href="../reference/pump_power.html">pump_power()</a></code> method.</p>
</div>
<div class="section level4">
<h4 id="exploring-the-impact-of-rho">Exploring the impact of rho<a class="anchor" aria-label="anchor" href="#exploring-the-impact-of-rho"></a>
</h4>
<p>The correlation of test statistics,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>,
is a critical parameter for how power will play out across the multiple
tests. For example, with Westfall-Young, we saw that the correlation can
improve our individual power, as compared to Bonferroni. We might not
know what will happen to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">2-</annotation></semantics></math>minimal
power, however: on one hand, correlated statistics make individual
adjustment less severe, and on the other correlation means we succeed or
fail all together. We can explore this question relatively easily by
letting <code>rho</code> vary as so:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gridRho</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/update_grid.html">update_grid</a></span><span class="op">(</span> <span class="va">pow</span>,</span>
<span>              MTP <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span> <span class="st">"BF"</span>, <span class="st">"WY-SD"</span> <span class="op">)</span>,</span>
<span>              rho <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">0.9</span>, by <span class="op">=</span> <span class="fl">0.15</span> <span class="op">)</span>,</span>
<span>              tnum <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>              B <span class="op">=</span> <span class="fl">3000</span> <span class="op">)</span></span></code></pre></div>
<p>We then plot our results.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span> <span class="va">gridRho</span> <span class="op">)</span></span></code></pre></div>
<p><img src="pump_demo_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;"></p>
<p>First, we see the benefit of the Westfall-Young single-step procedure
is minimal, as compared to Bonferroni. Second, the impact on individual
adjustment is flat, as anticipated. Third, across a very broad range of
rho, we maintain good
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">1-</annotation></semantics></math>minimal
power. Complete power climbs as correlation increases, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>−</mo></mrow><annotation encoding="application/x-tex">2-</annotation></semantics></math>minimal
power is generally unchanged.</p>
</div>
<div class="section level4">
<h4 id="exploring-the-impact-of-null-outcomes">Exploring the impact of null outcomes<a class="anchor" aria-label="anchor" href="#exploring-the-impact-of-null-outcomes"></a>
</h4>
<p>We finally explore varying the number of outcomes with no effects.
This exploration is an important way to hedge a design against the
possibility that some number of the identified outcomes are measured
poorly, or are simply not impacted by treatment. We use a grid search,
varying the number of outcomes that have no treatment impact via the
<code>numZero</code> design parameter:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gridZero</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/update_grid.html">update_grid</a></span><span class="op">(</span> <span class="va">pow</span>,</span>
<span>                         numZero <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">4</span>,</span>
<span>                         M <span class="op">=</span> <span class="fl">5</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span> <span class="va">gridZero</span>, nrow <span class="op">=</span> <span class="fl">1</span> <span class="op">)</span></span></code></pre></div>
<p><img src="pump_demo_files/figure-html/numzerogrid-compile-1.png" width="672" style="display: block; margin: auto;"></p>
<p>There are other ways of exploring the impact of weak or null effects
on some outcomes. In particular, the <code><a href="../reference/pump_power.html">pump_power()</a></code> and
<code><a href="../reference/pump_sample.html">pump_sample()</a></code> methods allow the researcher to provide an
MDES vector with different values for each outcome, including 0s for
some outcomes. The <code><a href="https://rdrr.io/r/graphics/grid.html" class="external-link">grid()</a></code> functions, by contrast, take a
single MDES value for the non-null outcomes, with a separate
specification of how many of the outcomes are 0. (This single value plus
<code>numZero</code> parameter also works with <code><a href="../reference/pump_power.html">pump_power()</a></code>
if desired.)</p>
</div>
<div class="section level4">
<h4 id="methods-for-pump-objects">Methods for <code>PUMP</code> objects<a class="anchor" aria-label="anchor" href="#methods-for-pump-objects"></a>
</h4>
<p>For user reference, we wrap up with a brief summary of methods that
can be applied to <code>PUMP</code>-generated objects.</p>
<p>The <code>PUMP</code> package returns two types of S3 objects.</p>
<ul>
<li>
<code>pumpresult</code> objects are returned from single scenario
calls: <code><a href="../reference/pump_power.html">pump_power()</a></code>, <code><a href="../reference/pump_mdes.html">pump_mdes()</a></code>,
<code><a href="../reference/pump_sample.html">pump_sample()</a></code>, and calls to <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code>.</li>
<li>
<code>pumpgridresult</code> objects are returned from grid calls:
<code><a href="../reference/pump_power_grid.html">pump_power_grid()</a></code>, <code><a href="../reference/pump_mdes_grid.html">pump_mdes_grid()</a></code>,
<code><a href="../reference/pump_sample_grid.html">pump_sample_grid()</a></code>, and calls to
<code><a href="../reference/update_grid.html">update_grid()</a></code>.</li>
</ul>
<p>The package has a variety of methods that can be called directly on
<code>pumpresult</code> objects.</p>
<ul>
<li>
<code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code> displays a concise summary of the most relevant
inputs and results of the call.</li>
<li>
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> prints a more extensive output, containing a
full summary of both the full list of inputs and the results of the
call.</li>
<li>
<code><a href="../reference/print_context.html">print_context()</a></code> provides a summary of the user inputs,
including the design and model and the parameter values.</li>
<li>
<code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> returns different plots tailored to whether the
results are for power, MDES, or sample size:
<ul>
<li>For power objects, it displays power across all power definitions
and MTPs.</li>
<li>For MDES and sample size objects, by default it displays a power
curve showing how power changes as sample size or MDES changes.</li>
<li>For MDES and sample size objects, the user can instead request a
diagnostic plot of the power search algorithm using
<code>type = "search"</code>.</li>
</ul>
</li>
<li>
<code><a href="../reference/power_curve.html">power_curve()</a></code> returns a data frame of power values over
a range of MDES or sample size values.</li>
<li>
<code><a href="../reference/pumpresult.html">search_path()</a></code> returns the search history of the search
algorithm for MDES and sample size calls.</li>
<li>
<code><a href="../reference/transpose_power_table.html">transpose_power_table()</a></code> converts a power table between
wide and long formats.</li>
<li>
<code><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame()</a></code> casts the object to a data frame of the
results.</li>
<li>
<code><a href="../reference/gen_sim_data.html">gen_sim_data()</a></code> generates a set of simulated data using
a data-generating process from the assumed design, model, and
parameters. For more details about functions to simulate data, see the
package vignette on simulating data.</li>
<li>
<code><a href="../reference/check_cor.html">check_cor()</a></code> checks the correlation between test
statistics using a simulation approach.</li>
</ul>
<p>Many of the above methods also apply to <code>pumpgridresult</code>
objects, although some are not relevant to grid objects. The main
difference in behavior between the <code>pumpresult</code> and
<code>pumpgridresult</code> objects is the output of <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code>
function. For an example of <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> called on a
<code>pumpresult</code> object, see Figure . In contrast, for an example
of <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> called on a grid object, see Figure . For grid
objects, the <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> function plots a facet wrap displaying
how power changes across all MTPs, power definitions, and varying
parameters provided during the grid call. If the user wants a smaller
set of results, they can specify a single <code>power.definition</code>,
or use <code>var.vary</code> to only plot variation in one parameter
value. If the grid call varied multiple parameters, then each plot
averages power across all other factors to plot main effects. For
example, in the ICC grid figure in “Exploring the impact of the ICC”,
the first plot averages over all values of <code>ICC.3</code> to show
how power varies with just <code>ICC.2</code>, and the second plot does
the opposite.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-RN27978" class="csl-entry">
Bloom, Howard S. 2006. <span>“The Core Analytics of Randomized
Experiments for Social Research.”</span> MDRC.
</div>
<div id="ref-RN23882" class="csl-entry">
Chen, J., J. Luo, K. Liu, and D. Mehrotra. 2011. <span>“On Power and
Sample Size Computation for Multiple Testing Procedures.”</span>
<em>Computational Statistics and Data Analysis</em> 55: 110–22.
</div>
<div id="ref-DNREPORT" class="csl-entry">
Corrin, W., S. Sepanik, R. Rosen, and A. Shane. 2016. <span>“Addressing
Early Warning Indicators: Interim Impact Findings from the Investing in
Innovation (I3) Evaluation of Diplomas Now.”</span> MDRC.
</div>
<div id="ref-RN4473" class="csl-entry">
Dong, Nianbo, and Rebecca Maynard. 2013. <span>“PowerUP!: A Tool for
Calculating Minimum Detectable Effect Sizes and Minimum Required Sample
Sizes for Experimental and Quasi-Experimental Design Studies.”</span>
<em>Journal of Research on Educational Effectiveness</em> 6 (1): 24–67.
</div>
<div id="ref-RN23878" class="csl-entry">
Dudoit, S., J. P. Shaffer, and J. C. Boldrick. 2003. <span>“Multiple
Hypothesis Testing in Microarray Experiments.”</span> <em>Statistical
Science</em> 18 (1): 71–103.
</div>
<div id="ref-GelmanHill2007" class="csl-entry">
Gelman, A., J. Hill, and M. Yajima. 2007. <em>Data Analysis Using
Regression and Multilevel/Hierarchical Models</em>. Cambridge University
Press.
</div>
<div id="ref-GELMANETAL2012" class="csl-entry">
———. 2012. <span>“Why We (Usually) Don’t Have to Worry about Multiple
Comparisons.”</span> <em>Journal of Research on Educational
Effectiveness</em> 5: 189–211.
</div>
<div id="ref-RN30153" class="csl-entry">
Hedges, Larry V., and Christopher Rhoads. 2010. <span>“Statistical Power
Analysis in Education Research.”</span> National Center for Special
Education Research. <a href="https://ies.ed.gov/use-work/resource-library/report/research-report/statistical-power-analysis-education-research" class="external-link">https://ies.ed.gov/use-work/resource-library/report/research-report/statistical-power-analysis-education-research</a>.
</div>
<div id="ref-Porter2018" class="csl-entry">
Porter, Kristin E. 2018. <span>“Statistical Power in Evaluations That
Investigate Effects on Multiple Outcomes: A Guide for
Researchers.”</span> <em>Journal of Research on Educational
Effectiveness</em> 11: 267–95.
</div>
<div id="ref-RN23884" class="csl-entry">
Raudenbush, S. W., H. Bloom, J. Spybrook, and A. Martinez. 2011.
<span>“Optimal Design with Empirical Information (OD+) (Version
3.0).”</span> <a href="https://wtgrantfoundation.org/optimal-design-with-empirical-information-od" class="external-link">https://wtgrantfoundation.org/optimal-design-with-empirical-information-od</a>.
</div>
<div id="ref-RN23748" class="csl-entry">
Schochet, Peter Z. 2008. <span>“Guidelines for Multiple Testing in
Impact Evaluations of Educational Interventions. Final Report.”</span>
Mathematica Policy Research, Inc. P.O. Box 2393, Princeton, NJ
08543-2393. <a href="https://eric.ed.gov/?id=ED502199" class="external-link">https://eric.ed.gov/?id=ED502199</a>.
</div>
<div id="ref-RN23881" class="csl-entry">
Senn, Stephen, and Frank Bretz. 2007. <span>“Power and Sample Size When
Multiple Endpoints Are Considered.”</span> <em>Pharmaceutical
Statistics</em> 6: 161–70. <a href="https://onlinelibrary.wiley.com/doi/10.1002/pst.301" class="external-link">https://onlinelibrary.wiley.com/doi/10.1002/pst.301</a>.
</div>
<div id="ref-MTSAS" class="csl-entry">
Westfall, Peter H, R. D. Tobias, and R. D. Wolfinger. 2011. <em>Multiple
Comparisons and Multiple Tests Using SAS</em>. The SAS Institute.
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luke Miratrix, Kristen Hunter, Zarni Htet, Kristin Porter, Institute of Education Sciences.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
